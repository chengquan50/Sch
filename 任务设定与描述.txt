
\subsection{实验环境与设置}

本章在第二章构建的“工艺参数优化仿真服务调度”环境中开展实验。所研究的优化过程源于液晶显示（LCD）产品工艺阶段的参数优化需求，该过程通常需要批量执行多条仿真工作流以筛选最优工艺配置。本案例的仿真任务参数参考了国内领先 LCD 制造企业京东方（BOE）的数据，并结合文献\cite{ren2019coding,li2022wholistic}的设定进行修正，以保证实验场景与行业实际一致。

在任务—资源配置方面，主要参数范围见表~\ref{tabel-cx-new-cn}。所有子任务的计算需求、IoT 数据输入量、软件许可资源、计算节点容量及链路通信条件，均在表~\ref{tabel-cx-new-cn}的范围内按正态分布随机采样生成，不同调度场景通过不同的随机种子加以区分。为覆盖多类任务规模与决策颗粒度，本章采用两种互补的 MDP 建模方式承载策略训练与验证：当任务规模中小、需精细控制“任务—节点”映射时，采用以显性节点选择为动作的建模；当任务规模较大、关注整体节拍与稳定性时，则采用以调度规则选择为动作的建模。两类 MDP 的状态—动作—奖励构造与训练细节将在后续方法验证部分分别阐述。

在动态扰动建模方面，四类扰动因子 $\phi_1 \!\sim\! \phi_4$ 分别用于触发子任务重执行、子任务插入、DAG 任务插入与计算节点宕机等动态事件。为与文献\cite{liu2019online,ma2021real}一致，$\phi_i$（$i=1,2,3,4$）均服从标准正态分布 $N(0,1)$ 且相互独立采样。在训练场景中，依据 $3\sigma$ 原则，将四类扰动的触发阈值统一设为 $\epsilon_i = 3$。  

\begin{table}[b]
\centering
\caption{工艺参数优化仿真场景参数配置}
\begin{tabular}{llllllll}
\cline{1-4}
\textbf{符号} & \multicolumn{1}{l|}{\textbf{取值范围}} & \textbf{符号} & \textbf{取值范围} \\ \cline{1-4}
$c_{ij}^{(com)}$ & \multicolumn{1}{l|}{2$\sim$3 $\times 10^6$ Megacycles} & $r_{ec}$ & 0.16$\sim$0.24 Gbps \\
$r^{(out)}_{ij}$ & \multicolumn{1}{l|}{8$\sim$16 GB} & $r_{ee}$ & 0.8$\sim$1.2 Gbps \\
$r_{ij}^{(in,k \in \mathcal{F_{ij}})}$ & \multicolumn{1}{l|}{1$\sim$7 GB} & $\varpi_0$ & $-100$ dBm \\
$\lvert \mathcal{E}_{ij} \rvert$ & \multicolumn{1}{l|}{\{3,4,5,6\}} & $q_{l}$ & 27$\sim$30 dBm \\
$w$ & \multicolumn{1}{l|}{0.4$\sim$0.6 Gbps} & $g_{e_{ij,k},ij}$ & 1.3$\sim$2.7 dB \\
$R_{mem,k}$ & \multicolumn{1}{l|}{64$\sim$512 GB} & $r_{ij}^{(mem)}$ & 4$\sim$64 GB \\
$R_{cpu,k}$ & \multicolumn{1}{l|}{8$\sim$48 核} & $r_{ij}^{(cpu)}$ & 4$\sim$16 核 \\
$f_{cpu,k}$ & \multicolumn{1}{l|}{2.4$\sim$4.5 GHz} & $\phi_1 \sim \phi_4$ & $-\infty \sim +\infty$ \\ \cline{1-4}
\end{tabular}
\label{tabel-cx-new-cn}
\end{table}


\subsection{MAML增强型方法的验证与分析}

为全面评估基于元学习的MAML-RPO调度策略在多任务动态场景下的表现，本节首先进行金鱼调度启发规则的隐式MDP建模，接着从训练表现、泛化能力以及任务规模可拓展性三个维度展开系统验证与对比分析，实验结果均基于多次重复实验与多环境仿真统计，确保结果的稳健性与代表性。为体现所提MAML-RPO方法在多任务动态扰动场景下的优势，本文选取了三类典型强化学习算法作为对比对象，分别为PPO、Trust Region Policy Optimization（TRPO）以及引入时序建模能力的Recurrent PPO（RPPO）。上述算法均在动态任务调度中具有广泛应用，具备代表性与对比价值。特别地，由于本文提出的方法在外循环策略更新时参考了TRPO的策略梯度学习机制，并在其基础上引入了元训练参数更新路径，因此将TRPO作为基础参考方法，有助于对比MAML结构带来的提升空间。同时，PPO作为当前最主流的策略优化算法，具备良好的训练效率与稳定性。而RPPO则可在时序特征较强的场景下展现出更优的策略记忆与状态感知能力。此外，上述三种算法均基于策略梯度框架，在优化原理、策略更新范式上具有高度的一致性，可视为同源改进算法。为保证公平性，所有算法均基于stable-baselines3库实现，并采用统一策略网络结构（隐藏层维度设置为$[128, 64]$），其余超参数设置保持常见且一致的配置。本节相关核心实现见公开仓库\url{https://github.com/chengquan50/DIFFUSION-discrete}。


\subsubsection{（1）基于调度启发规则的隐式动作MDP建模}

在隐式动作建模中，每个时间步 $t$ 并非仅对单个任务做出决策，而是需要对当前所有处于就绪状态的待调度任务进行统一分配。因此，智能体在每个时间步会从预定义的规则集合中选择一个动作 $a_t \in \{HR_0, HR_1, \dots, HR_8\}$，该动作将作为本轮所有待调度任务的排序与节点映射依据。动作的执行对象是数量不定的子任务批次而非单个任务，即在同一时间步内，所有待调度对象都会按照所选规则完成排序并依次分配至可用节点。

规则集合包含两类策略：动作 $HR_0$ 至 $HR_4$ 为基于确定性贪婪启发的排序与分配策略，这类规则直接根据任务或节点的特定属性（如处理时间、资源空闲率、传输延迟等）生成唯一的分配方案；动作 $HR_5$ 至 $HR_8$ 则为基于多候选方案筛选的策略，这类规则会先随机生成 $M$ 组可行的任务—节点映射方案，再根据对应的优化准则（如计算时间最短、资源利用率最高等）从中选择最优者。具体动作定义如表~\ref{table_act} 所示。
\begin{table}[htbp]
\centering
\caption{调度规则动作集定义}
\label{table_act}
\begin{tabular}{cl}
\toprule
\textbf{动作编号} & \textbf{规则描述} \\
\midrule
HR\_0 & 按最短处理时间（传输+执行）顺序调度任务 \\
HR\_1 & 优先调度至 CPU 空闲率最高的节点 \\
HR\_2 & 优先调度至内存空闲率最高的节点 \\
HR\_3 & 按预计仿真执行时间最短的顺序分配任务 \\
HR\_4 & 优先选择数据传输时间最小的节点 \\
HR\_5 & 从 $M$ 个随机方案中选择预计计算时间最短的方案 \\
HR\_6 & 从 $M$ 个随机方案中选择 CPU 空闲率方差最小的方案 \\
HR\_7 & 从 $M$ 个随机方案中选择总处理时间最短的方案 \\
HR\_8 & 从 $M$ 个随机方案中选择总处理时间方差最小的方案 \\
\bottomrule
\end{tabular}
\end{table}

在该建模下，状态$s_t$不再是任务或者资源级别特征的堆叠，而是以观测值形式替代系统状态，构建了一组可观测系统全局统计指标来表示调度环境。本节设计了如下19维归一化指标作为状态向量，如表~\ref{table_state} 所示：
\begin{table}[t]
\centering
\caption{系统观测指标构成）}
\label{table_state}
\begin{tabular}{ll}
\toprule
\textbf{State变量} & \textbf{定义} \\
\midrule
$State_0$ & 当前调度时刻的系统时钟时间 \\
$State_1$ & 系统空闲CPU占比 \\
$State_2$ & 所有计算节点空闲CPU占比的标准差 \\
$State_3$ & 云端节点的空闲CPU占比 \\
$State_4$ & 系统空闲内存占比 \\
$State_5$ & 所有节点空闲内存占比的标准差 \\
$State_6$ & 云节点的空闲内存占比 \\
$State_7$ & 未完成任务数与总任务数之比 \\
$State_8$ & 当前与下一时刻系统时间差 \\
$State_9$ & 正在执行任务的平均剩余处理时间 \\
$State_{10}$ & 正在执行任务的剩余时间标准差 \\
$State_{11}$ & 正在执行任务所需剩余CPU总量 \\
$State_{12}$ & 正在执行任务所需剩余内存总量 \\
$State_{13}$ & 待调度任务的平均预计计算时间 \\
$State_{14}$ & 待调度任务预计计算时间的标准差 \\
$State_{15}$ & 待调度任务的平均CPU需求 \\
$State_{16}$ & 待调度任务CPU需求的标准差 \\
$State_{17}$ & 待调度任务的平均内存需求 \\
$State_{18}$ & 待调度任务内存需求的标准差 \\
\bottomrule
\end{tabular}
\end{table}
所有状态特征均经归一化或者缩放处理，以统一输入尺度。

以最小化最大完工时间为目标，为将其转换成密集奖励，奖励函数考虑核心资源的实际利用效率，定义如下：
\begin{equation}
r_t = - NIC_t \cdot (Time^{(t)} - Time^{(t-1)})
\end{equation}
其中$NIC_t$为当前时间步的空闲CPU总量，$Time^{(t)}$表示系统当前调度时刻。

\subsubsection{（2）训练过程表现分析}

图~\ref{5-2-exp1_tu1} 展示了四类代表性深度强化学习算法在多次随机种子训练过程中的回报曲线平均表现，包括MAML-RPO、PPO、RPPO与TRPO，反映了各算法在多任务动态调度环境中的收敛速度、稳定性以及训练阶段的波动性差异。

从整体走势来看，MAML-RPO（图~\ref{5-2-exp1_tu1}(a)）在前期呈现出较为快速的上升趋势，且后期曲线趋于平稳，标准差较小，表现出良好的训练稳定性与稳健性。这种稳定提升得益于其基于元训练的参数初始化机制，使得策略在多任务切换中具备良好的迁移性，从而减少了训练波动。

PPO（图~\ref{5-2-exp1_tu1}(b)）虽然前期也能实现较快提升，但在多个阶段出现了显著的震荡波动，尤其在中后期（$1.0 \times 10^7$至$1.8 \times 10^7$步）呈现出大幅度的回报下降。这一现象表明该算法在面临任务动态扰动时策略容易失稳，难以维持持续的优化趋势。TRPO（图~\ref{5-2-exp1_tu1}(c)）的训练曲线则表现为早熟型特征，即前期收敛较快但迅速达到平台期，且曲线在$2.0 \times 10^6$步后基本不再变化，说明策略缺乏足够的探索能力，可能陷入局部最优。此外，其曲线几乎无波动，虽表现出极高稳定性，但也可能意味着策略优化空间受限。相比之下，RPPO（图~\ref{5-2-exp1_tu1}(d)）在引入时序特征提取模块后，训练曲线明显较为平滑，表明其在时序建模方面具有一定优势。然而，RPPO在后期同样出现了较长的性能停滞期，即在$5.0 \times 10^6$至$1.5 \times 10^7$步之间回报值维持在局部水平，且波动逐渐收敛，反映其策略过早收敛、泛化能力受限的问题。

综上，MAML-RPO在训练阶段展现出更强的稳定性与适应性，既具备稳定收敛趋势，也保留了策略的探索能力，避免陷入早期收敛困境。相较于其他基准方法，其曲线特征充分体现了元训练机制在多任务调度中的优势。为确保公平性，本文后续测试统一采用各算法在训练中表现最优的策略模型作为评估对象。

\begin{figure}[htbp]
\centering
\begin{minipage}{0.95\linewidth}
\subfigure[MAML-RPO]{
\includegraphics[width=0.47\linewidth,height=1.7in]{fig/5-2-training_mamlrpo.png}
}
\subfigure[PPO]{
\includegraphics[width=0.47\linewidth,height=1.7in]{fig/5-2-training_ppo.png}
}
\vskip\baselineskip
\subfigure[TRPO]{
\includegraphics[width=0.47\linewidth,height=1.7in]{fig/5-2-training_trpo.png}
}
\subfigure[RPPO]{
\includegraphics[width=0.47\linewidth,height=1.7in]{fig/5-2-training_rppo.png}
}
\end{minipage}
\caption{四种深度强化学习算法的训练回报曲线，基于多个任务环境的重复实验均值。}
\label{5-2-exp1_tu1}
\end{figure}



\subsubsection{泛化能力分析}
为了全面评估所提MAML-RPO算法在训练外部环境中的适应能力与鲁棒性，本文设计了涵盖多扰动情境与未见任务结构的泛化测试实验。测试任务集分为两部分：一部分为原始训练集中的30组任务场景；另一部分则包括60个从未见过的测试场景，并进一步覆盖了三类典型扰动类型（分别记为Dynamic1、Dynamic2与Dynamic3），每类扰动均涉及20种不同任务分布，充分模拟了现实工业调度中面临的复杂多变性。

在训练任务场景下，表~\ref{5-2-exp1_re1} 列出了各算法的测试表现。从平均值可见，MAML-RPO显著优于其他深度强化学习和启发式对比方法，在多组测试任务中均保持稳定的较低目标值。同时，图~\ref{5-2-exp1_tu2} 进一步刻画了MAML-RPO相较于其他算法在30组训练环境中的相对性能优势。图中以“Label-1”与“Label-30”分别表示单环境和30个用于MAML-RPO训练的环境下的平均性能差距比例。通过对实验数据的分析揭示了几个关键信息：1）基于DRL的算法在动态和多样化场景中表现出色，在30个训练环境中比启发式算法平均提高了约28.6％。2）某些DRL算法，例如PPO，在特定场景中表现出优异的调度性能，但容易过拟合，从而失去通用性。3）MAML-RPO在多种场景中表现出更广泛的适应性，比其他DRL算法提高了30.0％，比基于规则的算法提高了57.5％。4）MAML-RPO在多种任务环境中的元训练使其能够捕获通用的调度决策逻辑，从而在不同设置中表现出色。

在未见测试场景中，为更细粒度分析不同尺度的扰动对算法表现的影响，在三种动态设置下进行了性能测试。动态阈值$\epsilon_i$（i=1,2,3,4）分别满足 2$\sigma$、3$\sigma$ 和 4$\sigma$ 原则以模拟不同程度的扰动强度。我们在每种动态设置下，分别对 20 种不同的任务数据分布的算法性能进行了广泛的测试。由于数据量巨大，仅在图 \ref{exp2_re3} 中展示统计结果。三个子图（a）（b）（c）分别对应Dynamic1（4$\sigma$）、Dynamic2（3$\sigma$）与Dynamic3（2$\sigma$）三种情形。MAML-RPO在三个扰动强度下均展现出明显更低的目标值分布，其上下四分位间距（IQR）远小于其他方法，且具有较小的尾部离群点，显示出更强的抗扰动能力。相比之下，启发式算法HR6~HR8与部分基线DRL方法在扰动增强时目标值显著上升，且结果波动较大，鲁棒性较差。

此外，表~\ref{5-2-exp2_re2} 给出了MAML-RPO在三类扰动场景中相较于其他算法的平均性能提升比例。整体来看，其相较于其他深度强化学习方法（如PPO、RPPO、TRPO）平均提升约为20.1\%，而相较于各类启发式调度方法，平均提升幅度可达31.8\%。这一显著提升进一步说明，元训练机制有效增强了策略对分布外任务特征的提取与泛化能力。与此同时，对结果的波动性进行统计后发现，MAML-RPO在三类扰动下所得调度目标的标准差平均下降70.4\%，表明其不仅在性能均值上具有优势，在结果稳定性方面亦优于传统方法。

最后，为评估跨场景泛化能力，图~\ref{5-2-exp2_3} 统计了在60个未见测试任务环境中，各算法获得最优（即目标值最低）结果的频数。从图中可以直观观察到，MAML-RPO在三类扰动下共获得43次最优记录，遥遥领先于其他算法，尤其在Dynamic2与Dynamic3两个复杂环境中获得16与17次第一，展示出强大的分布外适应能力。而如PPO、TRPO与多数启发式算法仅在个别任务中取得领先，泛化能力有限，表现分散。

综上所述，上述结果充分验证了MAML-RPO在复杂任务扰动、跨场景测试及未见结构分布下的优越泛化能力与鲁棒性。这种能力不仅体现为平均性能的提升，还包括结果波动性的有效压制，是动态调度系统中实现实际部署和长期适应的关键保障。
\begin{table}[htbp]
\centering
\caption{训练任务场景下的测试性能统计}
\label{5-2-exp1_re1}
\begin{tabular}{c|ccccccc}
\hline
算法 & MAML-RPO & PPO & RPPO & TRPO & HR0 & HR1 & HR2 \\ \hline
30组平均 & 6439.8 & 8743.5 & 8490.3 & 7862.3 & 8490.3 & 9876.2 & 9943.3 \\
训练场景 & 5847.9 & 4719.4 & 9885.6 & 7839.1 & 8246.9 & 9685.4 & 9021.2 \\ \hline
算法 & HR3 & HR4 & HR5 & HR6 & HR7 & HR8 & \\ \hline
30组平均 & 9284.9 & 10500.5 & 10953.8 & 11383.6 & 9116.2 & 11711.3 & \\ \hline
训练场景 & 11083.3 & 8276.0 & 9685.4 & 7795.4 & 9885.6 & 6013.8 & \\ \hline
\end{tabular}
\end{table}

\begin{figure}[htbp]
\centering
\includegraphics[width=0.75\linewidth]{fig/5-2-exp1_2.png}
\caption{MAML-RPO相较于其他算法在训练场景中的性能领先比例。}
\label{5-2-exp1_tu2}
\end{figure}

\begin{figure}[htbp]
\centering
\begin{minipage}{0.95\linewidth}
\subfigure[Dynamic1]{
\includegraphics[width=0.31\linewidth,height=1.6in]{fig/5-2-exp2_11_boxplot.png}
}
\subfigure[Dynamic2]{
\includegraphics[width=0.31\linewidth,height=1.6in]{fig/5-2-exp2_12_boxplot.png}
}
\subfigure[Dynamic3]{
\includegraphics[width=0.31\linewidth,height=1.6in]{fig/5-2-exp2_13_boxplot.png}
}
\end{minipage}
\caption{三类动态扰动下算法调度性能分布，每类环境含20个不同任务分布。}
\label{5-2-exp2_re3}
\end{figure}

\begin{table}[htbp]
\centering
\caption{MAML-RPO相较各对比算法的平均性能提升比例}
\label{5-2-exp2_re2}
\begin{tabular}{l|ccc}
\toprule
\textbf{算法} & Dynamic 1（\%）& Dynamic 2（\%） & Dynamic 3（\%） \\
\midrule
PPO   & 15.9 & 35.6 & 14.5 \\
RPPO  & 14.7 & 30.0 & 12.7 \\
TRPO  & 14.4 & 30.7 & 12.6 \\
HR0   & 14.7 &  6.5 & 13.4 \\
HR1   & 22.4 & 16.0 & 41.5 \\
HR2   & 21.1 & 21.4 & 39.3 \\
HR3   & 29.0 & 20.0 & 26.2 \\
HR4   & 25.2 & 25.6 & 45.6 \\
HR5   & 33.3 & 28.5 & 51.3 \\
HR6   & 40.2 & 48.6 & 58.8 \\
HR7   & 26.1 & 11.6 & 21.5 \\
HR8   & 50.9 & 52.3 & 67.1 \\
\hline
平均值 & 25.7 & 27.2 & 33.7 \\
\bottomrule
\end{tabular}
\end{table}

\begin{figure}[htbp]
\centering
\includegraphics[width=0.75\linewidth]{fig/5-2-exp2_3.png}
\caption{60个测试环境中各算法取得最优的频次统计结果。}
\label{5-2-exp2_re4}
\end{figure}

\subsubsection{规模可拓展性分析}

基于启发式规则的动作建模方式使得算法具备良好的规模可扩展性。为进一步验证所提出的 MAML-RPO 算法在更大任务规模下的适应性与扩展能力，本文构建了四类典型问题规模场景，任务规模分别为 $300$、$450$、$700$ 与 $1000$，每一类规模下均生成 $30$ 个具有不同任务数据分布的测试场景，形成总计 $120$ 个待调度实例。

图~\ref{5-2-exp3_re2} 展示了各算法在不同任务规模下的平均调度目标值表现。从结果可以看出，MAML-RPO 在四类规模下均取得了最优的平均调度性能，特别是在任务规模扩大至 $700$ 和 $1000$ 时，其优势进一步凸显。相比之下，其余强化学习算法和启发式方法的性能随着任务规模增加出现不同程度的退化，表现出较强的任务依赖性和泛化限制。

\begin{figure}[htbp]
\centering
\includegraphics[width=0.75\linewidth]{fig/5-2-exp3_1.png}
\caption{MAML-RPO在四种问题规模下的平均调度性能。}
\label{5-2-exp3_re2}
\end{figure}

图~\ref{5-2-exp3_re3} 分别给出了各类任务规模下各算法在 $30$ 个任务场景中的调度性能箱线图。从图中可以观察到，MAML-RPO 不仅在平均性能上处于领先地位，在稳定性方面同样表现优异——其箱体高度与离群点数均明显低于其他算法，说明其在不同任务数据分布下保持了良好的鲁棒性。例如，在最大规模任务（任务数 $=1000$）的测试中，尽管其他算法的性能波动显著增加，MAML-RPO 依然保持了较低的中位数和较小的标准差，显示出强健的可拓展性。

\begin{table}[htbp]
\centering
\caption{不同任务规模下各算法的调度性能均值对比}
\label{exp3_re1}
\begin{tabular}{c|cccc}
\hline
任务数 & 300 & 450 & 700 & 1000 \\ \hline
MAML-RPO & 6439.8 & 8793.5 & 12979.6 & 16828.1 \\ 
PPO & 8743.5 & 11105.1 & 15180.0 & 21372.8 \\ 
RPPO & 8490.3 & 10892.6 & 16230.8 & 21645.8 \\ 
TRPO & 7862.3 & 10112.7 & 15707.8 & 20926.8 \\ 
HR0 & 8490.3 & 10928.2 & 16524.0 & 21625.7 \\ 
HR1 & 9876.2 & 11698.4 & 18440.8 & 22409.7 \\ 
HR2 & 9943.3 & 11475.1 & 18241.2 & 23026.9 \\ 
HR3 & 9284.9 & 12331.9 & 18973.5 & 24344.2 \\ 
HR4 & 10500.5 & 11337.7 & 17308.4 & 22644.1 \\ 
HR5 & 10953.8 & 12633.3 & 16779.4 & 20872.9 \\ 
HR6 & 11383.6 & 12856.3 & 17337.8 & 21631.1 \\ 
HR7 & 9116.2 & 10948.2 & 14477.4 & 18605.2 \\ 
HR8 & 11711.3 & 14519.9 & 19591.3 & 25322.7 \\ \hline
\end{tabular}
\end{table}

\begin{figure}[htbp]
\centering
\begin{minipage}{0.99\linewidth}
\subfigure[任务数=300]{
\includegraphics[width=0.47\linewidth,height=2in]{fig/5-2-exp3_14_boxplot.png}
}
\subfigure[任务数=450]{
\includegraphics[width=0.47\linewidth,height=2in]{fig/5-2-exp3_11_boxplot.png}
}
\vskip\baselineskip
\subfigure[任务数=700]{
\includegraphics[width=0.47\linewidth,height=2in]{fig/5-2-exp3_12_boxplot.png}
}
\subfigure[任务数=1000]{
\includegraphics[width=0.47\linewidth,height=2in]{fig/5-2-exp3_13_boxplot.png}
}
\end{minipage}
\caption{不同问题规模下各算法的性能分布}
\label{5-2-exp3_re3}
\end{figure}

总体来看，MAML-RPO 凭借其元训练机制和基于启发式编码的动作结构设计，在无需针对任务规模进行额外训练的前提下，展现出显著的跨规模适应能力。这一特性对于实际工业场景中面临的任务规模快速变化具有重要意义，显著提升了算法的通用性和部署效率。

\subsection{对比与讨论}

综合前述多维度实验结果可以看出，所提出的基于元强化学习的两类调度策略，在不同任务设定、动态扰动与任务规模下均表现出独特优势。与传统的单场景训练机制相比，这两类方法在跨场景泛化、动态适应性及运行稳定性方面均有明显提升，具备参考借鉴意义。

从动态扰动适应性来看，MDSA与MAML-RPO在扰动频率和扰动类型多变的条件下，均能保持稳定的调度性能，其性能下降幅度明显小于PPO、SAC、TRPO等主流基准算法。尤其在任务重执行、任务插入和节点宕机等高影响扰动下，二者均展现出更低的性能波动和更高的平均回报，体现了策略在非平稳环境下的稳健性。这一优势主要得益于第三章提出的多步状态特征融合机制与全局上下文感知结构，使得策略能够捕捉任务历史演化、资源使用趋势与扰动触发模式之间的时序依赖关系，从而实现对动态变化的前瞻性响应。

在跨场景泛化测试中，MAML-RPO在未见任务分布下的最优次数和平均性能提升幅度均显著高于对比算法，MDSA亦在大多数场景中保持领先。这说明多场景元训练结合结构化状态表征能够有效缓解训练与部署环境之间的分布偏移，使得策略在面对任务结构差异、资源配置变化时依然能够快速适应并维持较优性能。相比之下，传统单一场景训练的策略更容易在特定扰动模式或任务结构下过拟合，泛化能力有限。

在规模可拓展性方面，MAML-RPO在任务数扩展时依然维持最低的平均目标值和较小的性能方差，MDSA在长时间持续运行场景中亦保持稳定的任务完成速率。这表明基于元训练和上下文感知的结构能够在不重新训练的前提下平滑适应任务规模或运行时长的显著变化，这对于工业仿真服务平台在不同负载阶段的稳定运行具有重要意义。

综合上述结果可以得出结论：当面临动态事件频繁变化及任务设定差异显著的调度场景时，结合多场景训练的元强化学习方法，能够显著提升策略的鲁棒性与泛化能力，为构建面向工业实际的高可靠调度系统提供了可行路径。